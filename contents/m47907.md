---
title: "Practice Test 4"
layout: page
---


# 12.1 Linear Equations

251\. e. A, B, and C. * * *
{: data-type="newline"}

All three are linear equations of the form *y* = *mx* + *b*.

252\. Let *y* = the total number of hours required, and *x* the square footage, measured in units of 1,000. The equation is: *y* = *x* + 4

253\. Let *y* = the total payment, and *x* the number of students in a class. The equation is: *y* = 100(*x*) + 2,000

254\. Let *y* = the total cost of attendance, and *x* the number of years enrolled. The equation is: *y* = 3,000(*x*) + 500

# 12.2: Slope and Y-intercept of a Linear Equation

255\. The independent variable is the hours worked on a car. The dependent variable is the total labor charges to fix a car.

256\. Let *y* = the total charge, and *x* the number of hours required. The equation is: *y* = 55*x* + 75 * * *
{: data-type="newline"}

The slope is 55 and the intercept is 75.

257\. *y* = 55(3.5) + 75 = 267.50

258\. Because the intercept is included in both equations, while you are only interested in the difference in costs, you do not need to include the intercept in the solution. The difference in number of hours required is: 6.3 – 2.4 = 3.9. * * *
{: data-type="newline"}

Multiply this difference by the cost per hour: 55(3.9) = 214.5. * * *
{: data-type="newline"}

The difference in cost between the two jobs is $214.50.

# 12.3: Scatter Plots

259\. The *X* and *Y* variables have a strong linear relationship. These variables would be good candidates for analysis with linear regression.

260\. The *X* and *Y* variables have a strong negative linear relationship. These variables would be good candidates for analysis with linear regression.

261\. There is no clear linear relationship between the *X* and *Y* variables, so they are not good candidates for linear regression.

262\. The *X* and *Y* variables have a strong positive relationship, but it is curvilinear rather than linear. These variables are not good candidates for linear regression.

# 12.4: The Regression Equation

263\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mi>r</mi><mrow><mo>(</mo> <mrow> <mfrac> <mrow> <msub> <mi>s</mi> <mi>y</mi> </msub> </mrow> <mrow> <msub> <mi>s</mi> <mi>x</mi> </msub> </mrow> </mfrac> </mrow> <mo>)</mo></mrow><mo>=</mo><mn>0.73</mn><mrow><mo>(</mo> <mrow> <mfrac> <mrow> <mn>9.6</mn> </mrow> <mrow> <mn>4.0</mn> </mrow> </mfrac> </mrow> <mo>)</mo></mrow><mo>=</mo><mn>1.752</mn><mo>≈</mo><mn>1.75</mn> </mrow> </math>

264\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mi>a</mi><mo>=</mo><mover accent="true"> <mi>y</mi> <mo>¯</mo> </mover> <mo>−</mo><mi>b</mi><mover accent="true"> <mi>x</mi> <mo>¯</mo> </mover> <mo>=</mo><mn>141.6</mn><mo>−</mo><mn>1.752</mn><mo stretchy="false">(</mo><mn>68.4</mn><mo stretchy="false">)</mo><mo>=</mo><mn>21.7632</mn><mo>≈</mo><mn>21.76</mn> </mrow> </math>

265\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> <mo>=</mo><mn>21.76</mn><mo>+</mo><mn>1.75</mn><mo stretchy="false">(</mo><mn>68</mn><mo stretchy="false">)</mo><mo>=</mo><mn>140.76</mn> </mrow> </math>

# 12.5: Correlation Coefficient and Coefficient of Determination

266\. The coefficient of determination is the square of the correlation, or *r*<sup>2</sup>. * * *
{: data-type="newline"}

For this data, *r*<sup>2</sup> = (–0.56)2 = 0.3136 ≈ 0.31 or 31%. This means that 31 percent of the variation in fuel efficiency can be explained by the bodyweight of the automobile.

267\. The coefficient of determination = 0.32<sup>2</sup> = 0.1024. This is the amount of variation in freshman college GPA that can be explained by high school GPA. The amount that cannot be explained is 1 – 0.1024 = 0.8976 ≈ 0.90. So about 90 percent of variance in freshman college GPA in this data is not explained by high school GPA.

268\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mi>r</mi><mo>=</mo><msqrt> <mrow> <msup> <mi>r</mi> <mn>2</mn> </msup> </mrow> </msqrt> </math>

 * * *
{: data-type="newline"}

<math xmlns="http://www.w3.org/1998/Math/MathML"> <msqrt> <mrow> <mn>0.5</mn> </mrow> </msqrt> <mo>=</mo><mn>0.707106781</mn><mo>≈</mo><mn>0.71</mn> </math>

 * * *
{: data-type="newline"}

You need a correlation of 0.71 or higher to have a coefficient of determination of at least 0.5.

# 12.6: Testing the Significance of the Correlation Coefficient

269\. *H<sub>0</sub>*\: *ρ* = 0 * * *
{: data-type="newline"}

*H<sub>a</sub>*\: *ρ* ≠ 0

270\. <math xmlns="http://www.w3.org/1998/Math/MathML" display=""> <mrow> <mi>t</mi><mo>=</mo><mfrac> <mrow> <mi>r</mi><msqrt> <mrow> <mi>n</mi><mo>−</mo><mn>2</mn> </mrow> </msqrt> </mrow> <mrow> <msqrt> <mrow> <mn>1</mn><mo>−</mo><msup> <mi>r</mi> <mn>2</mn> </msup> </mrow> </msqrt> </mrow> </mfrac> <mo>=</mo><mfrac> <mrow> <mn>0.33</mn><msqrt> <mrow> <mn>30</mn><mo>−</mo><mn>2</mn> </mrow> </msqrt> </mrow> <mrow> <msqrt> <mrow> <mn>1</mn><mo>−</mo><msup> <mrow> <mn>0.33</mn> </mrow> <mn>2</mn> </msup> </mrow> </msqrt> </mrow> </mfrac> <mo>=</mo><mn>1.85</mn> </mrow> </math>

 * * *
{: data-type="newline"}

The critical value for *α* = 0.05 for a two-tailed test using the *t*<sub>29</sub> distribution is 2.045. Your value is less than this, so you fail to reject the null hypothesis and conclude that the study produced no evidence that the variables are significantly correlated. * * *
{: data-type="newline"}

Using the calculator function tcdf, the *p*-value is 2tcdf(1.85, 10^99, 29) = 0.0373. Do not reject the null hypothesis and conclude that the study produced no evidence that the variables are significantly correlated.

271\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mi>t</mi><mo>=</mo><mfrac> <mrow> <mi>r</mi><msqrt> <mrow> <mi>n</mi><mo>−</mo><mn>2</mn> </mrow> </msqrt> </mrow> <mrow> <msqrt> <mrow> <mn>1</mn><mo>−</mo><msup> <mi>r</mi> <mn>2</mn> </msup> </mrow> </msqrt> </mrow> </mfrac> <mo>=</mo><mfrac> <mrow> <mn>0.45</mn><msqrt> <mrow> <mn>25</mn><mo>−</mo><mn>2</mn> </mrow> </msqrt> </mrow> <mrow> <msqrt> <mrow> <mn>1</mn><mo>−</mo><msup> <mrow> <mn>0.45</mn> </mrow> <mn>2</mn> </msup> </mrow> </msqrt> </mrow> </mfrac> <mo>=</mo><mn>2.417</mn> </mrow> </math>

 * * *
{: data-type="newline"}

The critical value for *α* = 0.05 for a two-tailed test using the *t*<sub>24</sub> distribution is 2.064. Your value is greater than this, so you reject the null hypothesis and conclude that the study produced evidence that the variables are significantly correlated. * * *
{: data-type="newline"}

Using the calculator function tcdf, the p-value is 2tcdf(2.417, 10^99, 24) = 0.0118. Reject the null hypothesis and conclude that the study produced evidence that the variables are significantly correlated.

# 12.7: Prediction

272\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> <mo>=</mo><mn>25</mn><mo>+</mo><mn>16</mn><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo><mo>=</mo><mn>105</mn> </mrow> </math>

273\. Because the intercept appears in both predicted values, you can ignore it in calculating a predicted difference score. The difference in grams of fiber per serving is 6 – 3 = 3 and the predicted difference in grams of potassium per serving is (16)(3) = 48.

# 12.8: Outliers

274\. An outlier is an observed value that is far from the least squares regression line. A rule of thumb is that a point more than two standard deviations of the residuals from its predicted value on the least squares regression line is an outlier.

275\. An influential point is an observed value in a data set that is far from other points in the data set, in a horizontal direction. Unlike an outlier, an influential point is determined by its relationship with other values in the data set, not by its relationship to the regression line.

276\. The predicted value for y is: <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> <mo>=</mo><mn>5</mn><mo>+</mo><mn>0.3</mn><mi>x</mi><mo>=</mo><mn>5.6</mn> </mrow> </math>

. The value of 6.2 is less than two standard deviations from the predicted value, so it does not qualify as an outlier. * * *
{: data-type="newline"}

Residual for (2, 6.2): 6.2 – 5.6 = 0.6 (0.6 &lt; 2(0.4))

277\. The predicted value for y is: <math xmlns="http://www.w3.org/1998/Math/MathML"> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> </math>

 = 2.3 – 0.1(4.1) = 1.89. The value of 2.32 is more than two standard deviations from the predicted value, so it qualifies as an outlier. * * *
{: data-type="newline"}

Residual for (4.1, 2.34): 2.32 – 1.89 = 0.43 (0.43 &gt; 2(0.13))

# 13.1: One-Way ANOVA

278\. <span data-type="list" data-list-type="enumerated"> <span data-type="item">Each sample is drawn from a normally distributed population</span> <span data-type="item">All samples are independent and randomly selected.</span> <span data-type="item">The populations from which the samples are draw have equal standard deviations.</span> <span data-type="item">The factor is a categorical variable.</span> <span data-type="item">The response is a numerical variable.</span> </span>

279\. *H<sub>0</sub>*\: *μ*1 = *μ*2 = *μ*3 = *μ*4 * * *
{: data-type="newline"}

*H<sub>a</sub>*\: At least two of the group means *μ*1, *μ*2, *μ*3, *μ*4 are not equal.

280\. The independent samples *t*-test can only compare means from two groups, while one-way ANOVA can compare means of more than two groups.

281\. Each sample appears to have been drawn from a normally distributed populations, the factor is a categorical variable (method), the outcome is a numerical variable (test score), and you were told the samples were independent and randomly selected, so those requirements are met. However, each sample has a different standard deviation, and this suggests that the populations from which they were drawn also have different standard deviations, which is a violation of an assumption for one-way ANOVA. Further statistical testing will be necessary to test the assumption of equal variance before proceeding with the analysis.

282\. One of the assumptions for a one-way ANOVA is that the samples are drawn from normally distributed populations. Since two of your samples have an approximately uniform distribution, this casts doubt on whether this assumption has been met. Further statistical testing will be necessary to determine if you can proceed with the analysis.

# 13.2: The *F* Distribution

283\. *SS<sub>within</sub>* is the sum of squares within groups, representing the variation in outcome that cannot be attributed to the different feed supplements, but due to individual or chance factors among the calves in each group.

284\. *SS<sub>between</sub>* is the sum of squares between groups, representing the variation in outcome that can be attributed to the different feed supplements.

285\. *k* = the number of groups = 4 * * *
{: data-type="newline"}

*n*<sub>1</sub> = the number of cases in group 1 = 30 * * *
{: data-type="newline"}

*n* = the total number of cases = 4(30) = 120

286\. *SS<sub>total</sub>* = *SS<sub>within</sub>* + *SS<sub>between</sub>* so *SS<sub>between</sub>* = *SS<sub>total</sub>* – *SS<sub>within</sub>* * * *
{: data-type="newline"}

621.4 – 374.5 = 246.9

287\. The mean squares in an ANOVA are found by dividing each sum of squares by its respective degrees of freedom (*df*). * * *
{: data-type="newline"}

For *SS<sub>total</sub>*, *df* = *n* – 1 = 120 – 1 = 119. * * *
{: data-type="newline"}

For *SS<sub>between</sub>*, *df* = k – 1 = 4 – 1 = 3. * * *
{: data-type="newline"}

For *SS<sub>within</sub>*, *df* = 120 – 4 = 116. * * *
{: data-type="newline"}

*MS<sub>between</sub>* = <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mfrac> <mrow> <mn>246.9</mn> </mrow> <mn>3</mn> </mfrac> </mrow> </math>

 = 82.3 * * *
{: data-type="newline"}

*MS<sub>within</sub>* = <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mfrac> <mrow> <mn>374.5</mn> </mrow> <mrow> <mn>116</mn> </mrow> </mfrac> </mrow> </math>

 = 3.23

288\. <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <mi>F</mi><mo>=</mo><mfrac> <mrow> <mi>M</mi><msub> <mi>S</mi> <mrow> <mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi> </mrow> </msub> </mrow> <mrow> <mi>M</mi><msub> <mi>S</mi> <mrow> <mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi> </mrow> </msub> </mrow> </mfrac> <mo>=</mo><mfrac> <mrow> <mn>82.3</mn> </mrow> <mrow> <mn>3.23</mn> </mrow> </mfrac> <mo>=</mo><mn>25.48</mn> </mrow> </math>

289\. It would be larger, because you would be dividing by a smaller number. The value of *MS<sub>between</sub>* would not change with a change of sample size, but the value of *MS<sub>within</sub>* would be smaller, because you would be dividing by a larger number (*df<sub>within</sub>* would be 136, not 116). Dividing a constant by a smaller number produces a larger result.

# 13.3: Facts About the *F* Distribution

290\. All but choice c, –3.61. *F* Statistics are always greater than or equal to 0.

291\. As the degrees of freedom increase in an *F* distribution, the distribution becomes more nearly normal. Histogram *F*2 is closer to a normal distribution than histogram *F*1, so the sample displayed in histogram *F*1 was drawn from the *F*<sub>3,15</sub> population, and the sample displayed in histogram *F*2 was drawn from the *F*<sub>5,500</sub> population.

292\. Using the calculator function Fcdf, *p*-value = Fcdf(3.67, 1E, 3,50) = 0.0182. Reject the null hypothesis.

293\. Using the calculator function Fcdf, *p*-value = Fcdf(4.72, 1E, 4, 100) = 0.0016 Reject the null hypothesis.

# 13.4: Test of Two Variances

294\. The samples must be drawn from populations that are normally distributed, and must be drawn from independent populations.

295\. Let <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <msubsup> <mi>σ</mi> <mi>M</mi> <mn>2</mn> </msubsup> </mrow> </math>

 = variance in math grades, and <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <msubsup> <mi>σ</mi> <mi>E</mi> <mn>2</mn> </msubsup> </mrow> </math>

 = variance in English grades. * * *
{: data-type="newline"}

*H<sub>0</sub>*\: <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <msubsup> <mi>σ</mi> <mi>M</mi> <mn>2</mn> </msubsup> </mrow> </math>

 ≤ <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <msubsup> <mi>σ</mi> <mi>E</mi> <mn>2</mn> </msubsup> </mrow> </math>

 * * *
{: data-type="newline"}

*H<sub>a</sub>*\: <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <msubsup> <mi>σ</mi> <mi>M</mi> <mn>2</mn> </msubsup> </mrow> </math>

 &gt; <math xmlns="http://www.w3.org/1998/Math/MathML"> <mrow> <msubsup> <mi>σ</mi> <mi>E</mi> <mn>2</mn> </msubsup> </mrow> </math>

